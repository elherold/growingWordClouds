{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file explores if determining sensitivity by measuring the similarity of the input word to one or more social justice buzzword works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                           ability\n",
      "1                                       able-bodied\n",
      "2                                           ableism\n",
      "3      Aboriginal and Torres Strait Islander People\n",
      "4                                         Afrikaner\n",
      "                           ...                     \n",
      "171                                 white supremacy\n",
      "172                                       Whiteness\n",
      "173                                  whitesplaining\n",
      "174                                            woke\n",
      "175                                            yeke\n",
      "Name: lemma, Length: 176, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Get words from macht.sprache\n",
    "import pandas as pd\n",
    "input_words_en_de = pd.read_json('macht.sprache_words.json')\n",
    "\n",
    "\n",
    "input_words_en = input_words_en_de[input_words_en_de['lemma_lang'] == 'en']['lemma'].reset_index(drop=True)\n",
    "input_words_de = input_words_en_de[input_words_en_de['lemma_lang'] == 'de']['lemma'].reset_index(drop=True)\n",
    "print(input_words_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using pre-trained GloVe Twitter 25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up lexicon\n",
    "import gensim.downloader as api\n",
    "glove_vectors = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word:  ancestors\n",
      "--- buzzword similarities ---\n",
      "    similar_word  buzzword_similarity\n",
      "1         slaves             0.576186\n",
      "19          jews             0.513533\n",
      "16      believed             0.475029\n",
      "7   commandments             0.458929\n",
      "6        priests             0.420096\n",
      "0       prophets             0.411606\n",
      "5         unborn             0.401501\n",
      "13       orphans             0.401005\n",
      "8         greeks             0.398529\n",
      "17     glorified             0.388898\n",
      "11    sacrificed             0.370196\n",
      "3   missionaries             0.365527\n",
      "4       worships             0.359310\n",
      "10       witches             0.354220\n",
      "18     egyptians             0.334991\n",
      "14   forefathers             0.326398\n",
      "9    worshippers             0.323238\n",
      "2      disciples             0.309377\n",
      "12    worshipped             0.290851\n",
      "15      apostles             0.231268\n"
     ]
    }
   ],
   "source": [
    "input_word = input_words_en.iloc[7]  # The word you want to find similar words for\n",
    "n = 20  # The number of most similar words you want\n",
    "print(\"input word: \", input_word)\n",
    "\n",
    "# Find the n most similar words to the specified word\n",
    "most_similar_words = glove_vectors.most_similar(input_word, topn=n)\n",
    "\n",
    "# Print the most similar words and their similarity scores\n",
    "#print(\"--- most similar words ---\")\n",
    "#for similar_word, similarity in most_similar_words:\n",
    "#    print(f\"{similar_word}: {similarity}\")\n",
    "\n",
    "\n",
    "\n",
    "# Rank the list of similar words according to their similarity to a buzzword \n",
    "buzzword_similarities = []\n",
    "buzzwords = ['discrimination', 'power', 'political'] #(culture, hate)\n",
    "\n",
    "print(\"--- buzzword similarities ---\")\n",
    "for similar_word, _ in most_similar_words:\n",
    "    similarity = 0\n",
    "    for buzzword in buzzwords:\n",
    "        similarity = similarity + glove_vectors.similarity(similar_word, buzzword)\n",
    "    buzzword_similarities.append((similar_word, similarity/len(buzzwords)))\n",
    "    \n",
    "    \n",
    "df = pd.DataFrame(buzzword_similarities, columns=['similar_word', 'buzzword_similarity'])   \n",
    "df.sort_values(by=['buzzword_similarity'], ascending=False, inplace=True) \n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using self-trained word2vec model on reddit comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load pre-trained Word2Vec model.\n",
    "w2v = gensim.models.Word2Vec.load(\"word2vec_test.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word:  ancestors\n",
      "--- buzzword similarities ---\n",
      "   similar_word  buzzword_similarity\n",
      "10  forefathers             0.447843\n",
      "4          mede             0.432698\n",
      "18       iranic             0.414727\n",
      "16      seljuks             0.409344\n",
      "15    sumerians             0.396964\n",
      "11       serers             0.388622\n",
      "17    anatolian             0.383567\n",
      "3         medes             0.367793\n",
      "12      afghans             0.364946\n",
      "14   decendents             0.360088\n",
      "2       mongols             0.349856\n",
      "13        herat             0.349758\n",
      "9      persians             0.346132\n",
      "8   descendents             0.345633\n",
      "6         avars             0.344050\n",
      "7     assyrians             0.338645\n",
      "19       persia             0.301108\n",
      "1     descended             0.286256\n",
      "5      ancestor             0.238327\n",
      "0   descendants             0.232567\n"
     ]
    }
   ],
   "source": [
    "input_word = input_words_en.iloc[7]  # The word you want to find similar words for\n",
    "n = 20  # The number of most similar words you want\n",
    "print(\"input word: \", input_word)\n",
    "\n",
    "most_similar_words = w2v.most_similar(input_word, topn=n)\n",
    "\n",
    "# Print the most similar words and their similarity scores\n",
    "#print(\"--- most similar words ---\")\n",
    "#for similar_word, similarity in most_similar_words:\n",
    "#    print(f\"{similar_word}: {similarity}\")\n",
    "\n",
    "\n",
    "# Rank the list of similar words according to their similarity to a buzzword \n",
    "buzzword_similarities = []\n",
    "buzzwords = ['discrimination', 'power', 'political']\n",
    "\n",
    "print(\"--- buzzword similarities ---\")\n",
    "for similar_word, _ in most_similar_words:\n",
    "    similarity = 0\n",
    "    for buzzword in buzzwords:\n",
    "        similarity = similarity + w2v.similarity(similar_word, buzzword)\n",
    "    buzzword_similarities.append((similar_word, similarity/len(buzzwords)))\n",
    "    \n",
    "    \n",
    "df = pd.DataFrame(buzzword_similarities, columns=['similar_word', 'buzzword_similarity'])   \n",
    "df.sort_values(by=['buzzword_similarity'], ascending=False, inplace=True) \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing it for all macht.sprache entries using the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     index      input_word                                      similar_words\n",
      "0        0         ability  [customize, agressing, abilities, competency, ...\n",
      "7        7       ancestors  [descendants, descended, mongols, medes, mede,...\n",
      "10      10    antisemitism  [antisemtism, historiography, zionism, semitis...\n",
      "17      17       barbarian  [qajar, selfs, irani, kannadigas, motherland, ...\n",
      "18      18          beggar  [canuck, benben, appoloboy, preppy, hookers, g...\n",
      "30      30           bossy  [gutless, seriosly, stalks, douches, backstabb...\n",
      "36      36           chief  [deputy, caretaker, sardar, commander, adviser...\n",
      "40      40           color  [fffa, seashell, faff, lightgrey, ddcef, fff, ...\n",
      "43      43        coloured  [colored, darker, heterochromia, shade, shades...\n",
      "46      46         cracker  [dickface, muther, wog, nobhead, fcken, asswho...\n",
      "51      51        darkness  [shines, faceless, hath, sidious, humankind, d...\n",
      "52      52            deaf  [redneck, dunce, enraged, actully, nitwit, sho...\n",
      "54      54      disability  [prevention, illness, hypertension, prognosis,...\n",
      "56      56       diversity  [reflective, trends, professions, prevailing, ...\n",
      "58      58     enslavement  [chattel, evoke, colonialist, subjugated, mosl...\n",
      "61      61       ethnicity  [ashkenazi, ethnicities, kurd, ethnic, ethnica...\n",
      "62      62             fat  [jewfat, spotty, wheelchairi, sannse, jasenm, ...\n",
      "65      65         foreign  [sadr, diplomatic, overseas, government, gover...\n",
      "68      68          gender  [orientation, sexuality, heterosexual, sexual,...\n",
      "69      69         gentile  [glorification, martyr, ccp, supremacism, ster...\n",
      "71      71          gossip  [tweets, rumours, unconfirmed, tmz, rumors, ta...\n",
      "77      77        homeland  [nakba, yemen, samaria, bashar, rab, houthi, r...\n",
      "81      81       immigrant  [immigrants, slovaks, uae, predominately, ance...\n",
      "85      85     interracial  [legalization, effeminacy, shyster, adherent, ...\n",
      "87      87        intersex  [arousal, antipsychotics, dignified, prescribi...\n",
      "91      91           madam  [vandaliser, heartfelt, kbob, forgave, cenabee...\n",
      "92      92         mankind  [saviour, sins, omnipotence, manifestations, d...\n",
      "96      96         mastery  [conversational, undercut, dictating, rudiment...\n",
      "99      99        migrants  [subjugated, subjugation, liberia, lybia, bagr...\n",
      "100    100        mistress  [clooney, frend, granny, reaves, dearest, hag,...\n",
      "104    104          nation  [slovenes, sahrawi, slaughtered, afghans, coun...\n",
      "106    106         natives  [normans, africans, asians, mestizo, ancestory...\n",
      "111    111           nurse  [milly, wasilla, incarcerated, nurses, boardin...\n",
      "112    112             one  [least, swoop, fairer, ymb, adressed, betting,...\n",
      "113    113        oriental  [turko, phoenicians, zoroastrian, senegambia, ...\n",
      "117    117            pale  [haired, skinned, throated, colder, blonde, st...\n",
      "118    118         partner  [amanat, entrepreneur, transvestite, stared, b...\n",
      "119    119          patron  [pontiff, antioch, knighted, unveiled, clair, ...\n",
      "122    122      prostitute  [pimp, molested, lunsford, pimps, petifile, st...\n",
      "128    128          racial  [hygiene, stereotyping, ethno, profiling, ethn...\n",
      "139    139         refugee  [hostages, rafah, fsa, internment, cynically, ...\n",
      "141    141  respectability  [inspires, maudlin, propogate, lowers, saddeni...\n",
      "145    145         seminal  [systemics, refereed, mayr, tcm, arborsculptur...\n",
      "150    150         slavery  [chattel, slaves, manumission, enslavement, ri...\n",
      "151    151          slaves  [slave, indentured, chattel, enslaved, cubans,...\n",
      "153    153        spinster  [gloucestershire, brackenridge, breckinridge, ...\n",
      "154    154          spirit  [principle, abide, demiurge, sin, conscience, ...\n",
      "161    161           tribe  [amuca, panwar, epirus, wampanoag, descendant,...\n",
      "163    163           white  [black, mulatto, supremacist, caucasion, skinn...\n",
      "174    174            woke  [tipline, brewed, rubbed, eyeballs, distraught...\n",
      "Index(['index', 'input_word', 'similar_words'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "path_to_model =  os.path.join('models', 'word2vec_test.model')\n",
    "path_to_input_words = os.path.join('macht.sprache_input', 'macht.sprache_words.json')\n",
    "\n",
    "# Load model & macht.sprache words\n",
    "w2v = gensim.models.Word2Vec.load(\"word2vec_test.model\").wv\n",
    "\n",
    "input_words_en_de = pd.read_json(path_to_input_words)\n",
    "input_words_en = input_words_en_de[input_words_en_de['lemma_lang'] == 'en']['lemma'].reset_index(drop=True)\n",
    "input_words_de = input_words_en_de[input_words_en_de['lemma_lang'] == 'de']['lemma'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Generate lists of similar words to macht.sprache words\n",
    "nr_similar_words = 20 \n",
    "\n",
    "input_and_similar_words_en = pd.DataFrame(input_words_en.items(), columns=['index','input_word'])\n",
    "input_and_similar_words_en['similar_words'] = ''\n",
    "\n",
    "\n",
    "for index, item in input_words_en.items():\n",
    "    try: \n",
    "        most_similar_words = w2v.most_similar(item, topn=nr_similar_words)\n",
    "        #print(item, most_similar_words)\n",
    "        input_and_similar_words_en.at[index, 'similar_words'] = [tuple[0] for tuple in most_similar_words]  \n",
    "    except: \n",
    "        input_and_similar_words_en.at[index, 'similar_words'] = np.nan\n",
    "        # TODO: store words that are not in the lexicon to output them in the end?\n",
    "\n",
    "input_and_similar_words_en.dropna(inplace=True) # remove all the rows of input words that could not be found in the lexicon\n",
    "print(input_and_similar_words_en)\n",
    "print(input_and_similar_words_en.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        similar_word  sensitive_similarity\n",
      "168        extremism              0.249485\n",
      "2397  discriminatory              0.248769\n",
      "2349       profiling              0.243204\n",
      "1152    stereotyping              0.235874\n",
      "147         islamism              0.234729\n",
      "...              ...                   ...\n",
      "1749          reaves              0.047598\n",
      "1791             dax              0.045074\n",
      "1773            turd              0.044780\n",
      "996            mutha              0.041985\n",
      "1758            jill              0.025048\n",
      "\n",
      "[913 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter the lists for sensitive terms  \n",
    "# 1. Approach: calculate the similarity to social justice buzzwords\n",
    "# 2. Approach: ask an LLM to assign a sensitivity score\n",
    "\n",
    "# Rank the list of similar words according to their similarity to a buzzword \n",
    "buzzwords = ['discrimination', 'power', 'political']\n",
    "sensitive_words_df = pd.DataFrame(columns=['similar_word', 'sensitive_similarity'])\n",
    "\n",
    "\n",
    "\n",
    "for index, row in input_and_similar_words_en.iterrows():\n",
    "    similar_words = row['similar_words']\n",
    "    #print(similar_words)\n",
    "    for similar_word in similar_words: \n",
    "        sensitive_similarity = 0\n",
    "        for buzzword in buzzwords: \n",
    "            sensitive_similarity = sensitive_similarity + w2v.similarity(similar_word, buzzword)\n",
    "            # Weighting the sensitive_similarity, TODO: better weighting?\n",
    "            weighted_sensitive_similarity = sensitive_similarity/len(buzzwords)\n",
    "            sensitive_words_df.loc[len(sensitive_words_df)] = [similar_word, weighted_sensitive_similarity]\n",
    "\n",
    "# Make sure the newly found terms do not occur more than once in the output\n",
    "sensitive_words_df.drop_duplicates(subset=['similar_word'], inplace=True)\n",
    "sensitive_words_df.sort_values(by=['sensitive_similarity'], ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "# Output the list of new terms (with their sensitivity score)\n",
    "print(sensitive_words_df)\n",
    "sensitive_words_df.to_csv(\"similar_sensitive_words.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordclouds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
