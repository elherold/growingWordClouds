{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "with open('API_KEY', 'r') as file:\n",
    "    API_KEY = file.read().strip()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY\n",
    ")\n",
    "# defaults to getting the key using os.environ.get(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df1 = pd.read_csv('similar_sensitive_words.csv')\n",
    "df2 = pd.read_csv('sensitivity_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise the values of \"sensitivity_score\" column of df1 and df2 to range 0 and 1\n",
    "df1['sensitivity_score'] = (df1['sensitivity_score'] - df1['sensitivity_score'].min()) / (df1['sensitivity_score'].max() - df1['sensitivity_score'].min())\n",
    "df2['sensitivity_score'] = (df2['sensitivity_score'] - df2['sensitivity_score'].min()) / (df2['sensitivity_score'].max() - df2['sensitivity_score'].min())\n",
    "# join two df of same structure\n",
    "df = pd.concat([df1, df2])\n",
    "# in case of duplicates in similar_word, join values of \"input_word\" dont loose columns \"sensitivity_score\"\n",
    "df = df.groupby('similar_word').agg({'input_word': lambda x: ', '.join(set(', '.join(x).split(', '))), 'sensitivity_score': \"mean\"}).reset_index()\n",
    "# sort by sensitivity_score\n",
    "df = df.sort_values('sensitivity_score', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('word: discrimination', 'word_cloud_reference: racial'), ('word: profiling', 'word_cloud_reference: racial'), ('word: radicalism', 'word_cloud_reference: foreign'), ('word: extremism', 'word_cloud_reference: racial, antisemitism'), ('word: repression', 'word_cloud_reference: homeland')]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('joined_sensitive_words.csv')\n",
    "# each request should be just 5 terms\n",
    "n = 5\n",
    "\n",
    "df_chunks = [df[i:i + n] for i in range(0, df.shape[0], n)]\n",
    "\n",
    "# to be carefull of token limits drop most rows\n",
    "df_chunks = df_chunks[0:4]\n",
    "\n",
    "# convert the list of chunks to a list of lists turned into strings with the similar_word column\n",
    "# requests = [[str(l) for l in df_chunk[\"similar_word\"]] for df_chunk in df_chunks]\n",
    "requests = [\n",
    "    [\n",
    "        (\"word: \" + row[\"similar_word\"], \"word_cloud_reference: \" + row[\"input_word\"])\n",
    "        for _, row in df_chunk.iterrows()\n",
    "    ]\n",
    "    for df_chunk in df_chunks\n",
    "]\n",
    "\n",
    "print(f\"{requests[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     similar_word            input_word  sensitivity_score\n",
      "0  discrimination                racial           1.000000\n",
      "1       profiling                racial           0.881935\n",
      "2      radicalism               foreign           0.876245\n",
      "3       extremism  racial, antisemitism           0.863442\n",
      "4      repression              homeland           0.863442\n"
     ]
    }
   ],
   "source": [
    "print(df_chunks[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[175], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(file_content\u001b[38;5;241m+\u001b[39moutput)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# how many tokens were used\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     tok \u001b[38;5;241m=\u001b[39m \u001b[43mtok\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_tokens\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# add to csv file a line with the number of used tokens\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens_used.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo-preview\"\n",
    "# Choose the model to use\n",
    "modeel = gpt4\n",
    "\n",
    "english = \"English\"\n",
    "german = \"German\"\n",
    "# Choose the input language\n",
    "input_language = english\n",
    "\n",
    "# Choose language of system prompt to determine the output language\n",
    "sys_prompt_english = f\"\"\"Your role is to assess the sensitivity of a list of words provided in {input_language}. For each word, you will assign a sensitivity rating from 0 to 1, where 1 indicates high sensitivity. Additionally, you will get the word_cloud_reference word or words that contributed to our growing word cloud as help to find a potentially sensitive meaning. For each assessed word, you will provide a short analysis, encompassing a definition, a discussion on the sensitivity of the word, and propose options for translating it between English and German. Each paragraph should be concise, with a maximum of 350 tokens, ensuring insightful analysis while using language respectfully and accurately, highlighting cultural and contextual nuances.\n",
    "        \n",
    "    Incorporate the principles and goals of macht.sprache. to support users in translating more sensitively between German and English. This includes recognizing and addressing linguistic discrimination, promoting expressions that challenge such discrimination, and fostering awareness for the sensitive handling of political terms in translations. Emphasize the importance of continuity, collaboration, creativity, and accessibility in this process. Acknowledge that while macht.sprache. aims to guide users, it cannot assume responsibility for the sensitivity of translations by individuals, underscoring the importance of self-education.\n",
    "    \n",
    "    Consider the perspectives provided by macht.sprache. on recognizing power and privileges, increasing awareness for justice, and choosing words that minimize harm over those that cause it. All translation decisions are political, and this perspective should guide the sensitivity assessment and translation options provided. The collaborative and ongoing nature of macht.sprache., its foundation in diverse expert contributions, and its commitment to pragmatism, accessibility, and creativity in translations are integral to your analysis and recommendations.\n",
    "    \n",
    "    Outputs should be formatted as JSON, including the following keys for each word in the list: \"word\", \"sensitivity_rating\", \"definition\", and \"translation_options\". Each \"translation_options\" entry should contain a list of 4 \"options\", each with its \"nuance\".\"\"\"\n",
    "\n",
    "\n",
    "sys_prompt_german = \"\"\"Ihre Aufgabe besteht darin, die Sensitivität einer Liste von Wörtern, die in Deutsch oder Englisch vorliegen, zu beurteilen. Sie sollen für jedes Wort eine Sensitivitätsbewertung zwischen 0 und 1 vergeben, wobei 1 die höchste Sensitivität darstellt. Zusätzlich erhalten Sie das ursprüngliche Eingabewort, das zu unserer sich erweiternden Wortwolke beiträgt, um möglicherweise sensible Bedeutungen identifizieren zu können. Für jedes beurteilte Wort sollen Sie eine knappe Analyse liefern, die eine Definition, ein Rating der Sensitivität des Wortes und Empfehlungen für Übersetzungen zwischen Englisch und Deutsch umfasst. Jeder Absatz sollte mit einem maximum von 350 token kurz und bündig sein. Ihre Analyse sollte einsichtsreich sein und die Sprache respektvoll und präzise nutzen, wobei kulturelle und kontextuelle Feinheiten berücksichtigt werden.\n",
    "\n",
    "Beziehen Sie die Prinzipien und Ziele von macht.sprache. ein, um Nutzern zu helfen, zwischen Deutsch und Englisch sensibler zu übersetzen. Dies schließt die Erkennung und Bekämpfung von sprachlicher Diskriminierung ein, das Eintreten für Ausdrücke, die dieser Diskriminierung entgegenwirken, und das Schaffen von Bewusstsein für den sensiblen Umgang mit politischen Begriffen in Übersetzungen. Unterstreichen Sie die Wichtigkeit von Beständigkeit, Zusammenarbeit, Kreativität und Zugänglichkeit in diesem Prozess. Machen Sie deutlich, dass macht.sprache. zwar Orientierung bietet, jedoch nicht die Verantwortung für die Sensitivität individueller Übersetzungen übernehmen kann, was die Bedeutung von Eigenverantwortung hervorhebt.\n",
    "\n",
    "Berücksichtigen Sie die Ansichten von macht.sprache. zur Anerkennung von Machtverhältnissen und Privilegien, zur Förderung eines Bewusstseins für Gerechtigkeit und zur Auswahl von Wörtern, die weniger Schaden anrichten, anstatt zu verletzen. Jede Entscheidung bei der Übersetzung ist politisch, und diese Perspektive sollte Ihre Bewertung der Sensitivität und die vorgeschlagenen Übersetzungsmöglichkeiten leiten. Die kooperative und fortlaufende Natur von macht.sprache., basierend auf dem Input verschiedener Experten und dem Engagement für Pragmatismus, Zugänglichkeit und Kreativität in der Übersetzung, sollte ein zentraler Bestandteil Ihrer Analyse und Empfehlungen sein.\n",
    "\n",
    "Die Ergebnisse sollen auf deutsch geschrieben und als JSON formatiert werden, mit den folgenden keys für jedes Wort in der Liste: \"word\" (Wort), \"sensitivity_rating\" (Sensitivitätsbewertung), \"definition\" und \"translation_options\" (Übersetzungsoptionen). Jeder Eintrag bei \"translation_options\" soll eine Liste von vier \"options\" (Optionen) sein, und jede Übersetzungsoption soll weiter mit ihrer eigenen \"nuance\" (Feinheit) beschrieben werden.\"\"\"\n",
    "\n",
    "# The number of tokens used in the completion\n",
    "tok = None\n",
    "\n",
    "# Specify the file name to append to\n",
    "file_name = \"output.json\"\n",
    "\n",
    "# looping over all chunks of the list and send API requests with thourough instructions\n",
    "for request in requests[1:2]:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=gpt4,\n",
    "        # response_format={ \"type\": \"json_object\" }, # argument that forces the response to be a json object but this seems to not work on a list of words as requests\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"{sys_prompt_german}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{request}\"},\n",
    "        ],\n",
    "        # low temperature because we want consistent output\n",
    "        temperature=0.1,\n",
    "    )\n",
    "\n",
    "    # before appending to the file delete anything before the first \"[\" and after the last \"]\"\n",
    "    txt = \"[\" + completion.choices[0].message.content.split('[', 1)[1]\n",
    "    # Split the string into a list using '[' as the separator\n",
    "    parts = txt.split(']')\n",
    "    # Reconstruct the string\n",
    "    output = ']'.join(parts[0:-1]) + ']'\n",
    "\n",
    "    file_content = None\n",
    "    # if file is not empty replace last character with a comma\n",
    "    with open(file_name, 'r') as file:\n",
    "        # Read the file\n",
    "        file_content = file.read()\n",
    "    \n",
    "    # if file is not empty replace last \"[\" with a comma\n",
    "    if file_content:\n",
    "        file_content = file_content[:-2] + \",\"\n",
    "    else:\n",
    "        file_content = \"\"\n",
    "    \n",
    "    # Open the file in append mode to add additional content instead of overwriting the content\n",
    "    with open(file_name, 'w') as file:\n",
    "        # Use the write() method to write the string to the file\n",
    "        file.write(file_content+output)\n",
    "\n",
    "    # how many tokens were used\n",
    "    tok = tok + completion.usage.total_tokens\n",
    "\n",
    "\n",
    "# add to csv file a line with the number of used tokens\n",
    "with open('tokens_used.csv', 'a') as file:\n",
    "    file.write(f\"{tok}\\n\")\n",
    "\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2955"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
